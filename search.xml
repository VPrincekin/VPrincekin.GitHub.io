<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>强化学习入门算法: QLearning &amp; Sarsa</title>
    <url>/2020/08/20/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AE%97%E6%B3%95-QLearning-Sarsa/</url>
    <content><![CDATA[<h2 id="QLearning-off-policy"><a href="#QLearning-off-policy" class="headerlink" title="QLearning (off-policy)"></a>QLearning (off-policy)</h2><h3 id="整体学习流程："><a href="#整体学习流程：" class="headerlink" title="整体学习流程："></a>整体学习流程：</h3><p>一次循环</p>
<ul>
<li><p>初始化环境</p>
</li>
<li><p>当前回合 （循环，直到当前回合结束）</p>
<ul>
<li><p>根据当前 S， 选择动作 a。(动作选择规则：1.随机选择。2.根据当前 S 最大的 a选择。)</p>
</li>
<li><p>在环境中采取行为 a, 获得下一个 S_ 和 R, 以及是否终止。</p>
</li>
<li><p>更新 Q Table:</p>
<p><img src="http://latex.codecogs.com/svg.latex?q_%7B%5C_predict%7D=Q%5Bs,a%5D"></p>
<p><img src="http://latex.codecogs.com/svg.latex?q_%7B%5C_target%7D=R+%5Cgamma*Q%5Bs%5C_,:%5D_%7Bmax%7D"></p>
<p><img src="http://latex.codecogs.com/svg.latex?Q(s,a)%5E%7Bnew%7D=Q(s,a)+%5Calpha*(q_%7B%5C_target%7D-q_%7B_predict%7D)"></p>
<p><img src="http://latex.codecogs.com/svg.latex?Q(s,a)%5E%7Bnew%7D=Q(s,a)+%5Calpha*(R+%5Cgamma*Q%5Bs%5C_,:%5D_%7Bmax%7D-Q%5Bs,a%5D)"></p>
<p>其中 alpha 代表学习率，来决定这次的误差有多少是要被学习的。gamma 代表对未来奖励的衰减值，来决定这次获得奖励的大小。</p>
</li>
<li><p>将 S_ 赋给 S, 继续当前循环。</p>
</li>
<li><p>判断当前回合是否结束。</p>
</li>
</ul>
</li>
</ul>
<h2 id="Sarsa-on-policy"><a href="#Sarsa-on-policy" class="headerlink" title="Sarsa (on-policy)"></a>Sarsa (on-policy)</h2><h3 id="整体学习流程：-1"><a href="#整体学习流程：-1" class="headerlink" title="整体学习流程："></a>整体学习流程：</h3><p>一次循环</p>
<ul>
<li><p>初始化环境</p>
</li>
<li><p>根据当前 S， 选择动作 a。(动作选择规则：1.随机选择。2.根据当前 S 最大的 a选择。)</p>
</li>
<li><p>当前回合 （循环，直到当前回合结束）</p>
<ul>
<li><p>在环境中采取行为 a, 获得下一个 S_ 和 R, 以及是否终止。</p>
</li>
<li><p>根据下一个 S_, 选择动作 a_。(动作选择规则同上)</p>
</li>
<li><p>更新 Q Table:</p>
<p><img src="http://latex.codecogs.com/svg.latex?q_%7B%5C_predict%7D=Q%5Bs,a%5D"></p>
<p><img src="http://latex.codecogs.com/svg.latex?q_%7B%5C_target%7D=R+%5Cgamma*Q%5Bs%5C_,a%5C_%5D"></p>
<p><img src="http://latex.codecogs.com/svg.latex?Q(s,a)%5E%7Bnew%7D=Q(s,a)+%5Calpha*(q_%7B%5C_target%7D-q_%7B_predict%7D)"></p>
<p><img src="http://latex.codecogs.com/svg.latex?Q(s,a)%5E%7Bnew%7D=Q(s,a)+%5Calpha*(R+%5Cgamma*Q%5Bs%5C_,a%5C_%5D-Q%5Bs,a%5D)"></p>
<p>其中 alpha 代表学习率，来决定这次的误差有多少是要被学习的。gamma 代表对未来奖励的衰减值，来决定这次获得奖励的大小。</p>
</li>
<li><p>将 S_ 赋给 S, a_ 赋给 a, 继续当前循环。</p>
</li>
<li><p>判断当前回合是否结束。</p>
</li>
</ul>
</li>
</ul>
<h3 id="与-QLearning-的区别："><a href="#与-QLearning-的区别：" class="headerlink" title="与 QLearning 的区别："></a>与 QLearning 的区别：</h3><ul>
<li>Sarsa 是说到做到型, 也叫 on-policy, 在线学习, 学着自己在做的事情. 而 QLearning 是说到但并不一定做到, 所以它也叫作 Off-policy, 离线学习. 而因为有了 maxQ, Q-learning 也是一个特别勇敢的算法. Q learning 机器人 永远都会选择最近的一条通往成功的道路, 不管这条路会有多危险. 而 Sarsa 则是相当保守, 他会选择离危险远远的, 完成任务是次要的, 保住自己的小命才是第一位. 这就是使用 Sarsa 方法的不同之处.</li>
<li>Sarsa 在当前 state 已经想好了 state 对应的 action, 而且想好了下一个 state_ 和下一个 action_ (Qlearning 还没有想好下一个 action_)。更新 Q(s,a) 的时候基于的是下一个 Q(s_, a_)(Qlearning 是基于 maxQ(s_))。</li>
<li>Sarsa 相对于 Qlearning, 更加的胆小. 因为 Qlearning 永远都是想着 maxQ 最大化, 因为这个 maxQ 而变得贪婪, 不考虑其他非 maxQ 的结果. 我们可以理解成 Qlearning 是一种贪婪, 大胆, 勇敢的算法, 对于错误, 死亡并不在乎. 而 Sarsa 是一种保守的算法, 他在乎每一步决策, 对于错误和死亡比较铭感. 这一点我们会在可视化的部分看出他们的不同. 两种算法都有他们的好处, 比如在实际中, 你比较在乎机器的损害, 用一种保守的算法, 在训练时就能减少损坏的次数.</li>
</ul>
<h2 id="Sarsa-lambda-on-policy"><a href="#Sarsa-lambda-on-policy" class="headerlink" title="Sarsa-lambda (on-policy)"></a>Sarsa-lambda (on-policy)</h2><h3 id="整体学习流程：-2"><a href="#整体学习流程：-2" class="headerlink" title="整体学习流程："></a>整体学习流程：</h3><p>一次循环</p>
<ul>
<li><p>初始化环境</p>
</li>
<li><p>根据当前 S， 选择动作 a。(动作选择规则：1.随机选择。2.根据当前 S 最大的 a选择。)</p>
</li>
<li><p>初始化 eligibility_trace(动作跟踪)</p>
</li>
<li><p>当前回合 （循环，直到当前回合结束）</p>
<ul>
<li><p>在环境中采取行为 a, 获得下一个 S_ 和 R, 以及是否终止。</p>
</li>
<li><p>根据下一个 S_, 选择动作 a_。(动作选择规则同上)</p>
</li>
<li><p>更新 Q Table:</p>
<p>对于经历过的 (s, a) state-action, 我们让他+1, 证明他是得到 reward 路途中不可或缺的一环。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line"># Method 1:</span><br><span class="line">eligibility_trace.loc[s, a] +&#x3D; 1</span><br><span class="line"># Method 2:</span><br><span class="line">eligibility_trace.loc[s, :] *&#x3D; 0</span><br><span class="line">eligibility_trace.loc[s, a] &#x3D; 1</span><br></pre></td></tr></table></figure>

<p><img src="http://latex.codecogs.com/svg.latex?q_%7B%5C_predict%7D=Q%5Bs,a%5D"></p>
<p><img src="http://latex.codecogs.com/svg.latex?q_%7B%5C_target%7D=R+%5Cgamma*Q%5Bs%5C_,a%5C_%5D"></p>
<p><img src="http://latex.codecogs.com/svg.latex?Q_%7Btable%7D%5E%7Bnew%7D=Q_%7Btable%7D+%5Calpha*(q_%7B%5C_target%7D-q_%7B_predict%7D)*eligibility%5C_trace"></p>
<p><img src="http://latex.codecogs.com/svg.latex?Q_%7Btable%7D%5E%7Bnew%7D=Q_%7Btable%7D+%5Calpha*(R+%5Cgamma*Q%5Bs%5C_,a%5C_%5D-Q%5Bs,a%5D)*eligibility%5C_trace"></p>
<p>eligibility_trace 的值随着时间衰减, 离获取 reward 越远的步, 他的”不可或缺性”越小。</p>
<p>eligibility_trace = eligibility_trace * gamma * lambda</p>
<p>其中 alpha 代表学习率，来决定这次的误差有多少是要被学习的。gamma 代表对未来奖励的衰减值，来决定这次获得奖励的大小。lambda 代表的脚步衰减值，0 为单步更新，1 为回合更新。</p>
</li>
<li><p>将 S_ 赋给 S, a_ 赋给 a, 继续当前循环。</p>
</li>
<li><p>判断当前回合是否结束。</p>
</li>
</ul>
</li>
</ul>
]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>QLearning</tag>
        <tag>Sarsa</tag>
      </tags>
  </entry>
  <entry>
    <title>深度强化学习-DQN</title>
    <url>/2020/08/24/%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B9%8BDQN/</url>
    <content><![CDATA[<h2 id="Deep-Q-Network"><a href="#Deep-Q-Network" class="headerlink" title="Deep Q Network"></a>Deep Q Network</h2><h3 id="整体学习流程"><a href="#整体学习流程" class="headerlink" title="整体学习流程"></a>整体学习流程</h3><p>一次循环</p>
<ul>
<li>初始化环境</li>
<li>当前回合 （循环，直到当前回合结束）<ul>
<li>刷新环境</li>
<li>根据当前 S， 选择动作 a.(动作选择规则：1.随机选择.2.根据当前 S 在 eval_net 中最大的 a 选择.)</li>
<li>在环境中采取行为 a, 获得下一个 S_ 和 R, 以及是否终止.</li>
<li>DQN 存储记忆.(总 memory 大小是固定的, 如果超出总大小, 旧 memory 就被新 memory 替换)</li>
<li>判断当前是否需要学习 (先累积一些记忆再开始学习):<ul>
<li>DQN学习:<ul>
<li>检查是否需要替换 target_net 参数.</li>
<li>从 memory 中随机抽取 batch_size 记忆</li>
<li>获取 q_next(target_net 产生的 q)和 q_eval(eval_net 产生的 q). </li>
<li>q_next, q_eval 包含所有 action 的值, 而我们需要的只是已经选择好的 action 的值, 其他的并不需要. 所以我们将其他的 action 值全变成 0, 将用到的 action 误差值 反向传递回去, 作为更新依据. </li>
<li>q_target = reward + gamma * max(q_next) (reward 来自于当前批次的 batch_memory)</li>
<li>根据误差((q_target-q_eval)^2), 训练 eval_net, 得到误差 cost.</li>
<li>逐渐增加 epsilon, 降低行为的随机性.</li>
</ul>
</li>
</ul>
</li>
<li>将 S_ 赋给 S, 继续当前循环.</li>
<li>判断当前回合是否结束.</li>
</ul>
</li>
</ul>
<h3 id="loss-function-的构造"><a href="#loss-function-的构造" class="headerlink" title="loss function 的构造"></a>loss function 的构造</h3><p><img src="/2020/08/24/%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B9%8BDQN/dqn_loss_function.png" alt="image"></p>
]]></content>
      <categories>
        <category>强化学习</category>
      </categories>
      <tags>
        <tag>DQN</tag>
      </tags>
  </entry>
  <entry>
    <title>发生在逻辑回归之前的故事.md</title>
    <url>/2020/10/27/%E5%8F%91%E7%94%9F%E5%9C%A8%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B9%8B%E5%89%8D%E7%9A%84%E6%95%85%E4%BA%8B/</url>
    <content><![CDATA[<h2 id="Gaussian-distribution-高斯分布"><a href="#Gaussian-distribution-高斯分布" class="headerlink" title="Gaussian distribution (高斯分布)"></a>Gaussian distribution (高斯分布)</h2>]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>Logistic</tag>
      </tags>
  </entry>
  <entry>
    <title>机器学习的问题处理.md</title>
    <url>/2020/11/09/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%97%AE%E9%A2%98%E5%A4%84%E7%90%86/</url>
    <content><![CDATA[]]></content>
      <categories>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>tutorials</tag>
      </tags>
  </entry>
  <entry>
    <title>编写自己的python包上传到pypi</title>
    <url>/2020/11/20/%E7%BC%96%E5%86%99%E8%87%AA%E5%B7%B1%E7%9A%84python%E5%8C%85%E4%B8%8A%E4%BC%A0%E5%88%B0pypi/</url>
    <content><![CDATA[<h3 id="1-创建包文件"><a href="#1-创建包文件" class="headerlink" title="1. 创建包文件"></a>1. 创建包文件</h3><p>您现在将创建一些文件来打包此项目并准备分发。创建下面列出的新文件 - 您将在以下步骤中向其添加内容。(以 wdhtools 为例)</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#x2F;wdhtools</span><br><span class="line">    &#x2F;wdhtools</span><br><span class="line">        __init__.py</span><br><span class="line">    LICENSE</span><br><span class="line">    REAEME.md</span><br><span class="line">    setup.py</span><br></pre></td></tr></table></figure>
<h3 id="2-创建-setup-py"><a href="#2-创建-setup-py" class="headerlink" title="2. 创建 setup.py"></a>2. 创建 setup.py</h3><p>setup.py 是 setuptools 的构建脚本。它告诉 setuptools 你的包（例如名称和版本）以及要包含的代码文件。</p>
<ul>
<li>打开 setup.py 并输入以下内容。您应该更新软件包名称以包含你的用户名。<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">from __future__ import print_function</span><br><span class="line">from setuptools import setup, find_packages</span><br><span class="line"></span><br><span class="line">setup(</span><br><span class="line">    name&#x3D;&#39;wdhtools&#39;,</span><br><span class="line">    version&#x3D;&#39;1.2.0&#39;,</span><br><span class="line">    author&#x3D;&#39;wdh&#39;,</span><br><span class="line">    packages&#x3D;find_packages(),</span><br><span class="line">    install_requires&#x3D;[&#39;pandas&#39;, &#39;sklearn&#39;, &#39;xgboost&#39;, &#39;matplotlib&#39;, &#39;seaborn&#39;, &#39;featexp&#39;]</span><br><span class="line">)</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h3 id="3-创建-REAEME-md"><a href="#3-创建-REAEME-md" class="headerlink" title="3. 创建 REAEME.md"></a>3. 创建 REAEME.md</h3><p>打开 README.md 并输入内容。可以自定义此项，放入一些对项目的介绍。此处不作介绍。</p>
<h3 id="4-创建许可证"><a href="#4-创建许可证" class="headerlink" title="4. 创建许可证"></a>4. 创建许可证</h3><p>上传到 Python Package Index 的每个包都包含许可证，这一点很重要。这告诉用户安装你的软件包可以使用您的软件包的条款。有关选择许可证的帮助，请访问 <a href="https://choosealicense.com/%E3%80%82%E9%80%89%E6%8B%A9%E8%AE%B8%E5%8F%AF%E8%AF%81%E5%90%8E%EF%BC%8C%E6%89%93%E5%BC%80">https://choosealicense.com/。选择许可证后，打开</a> LICENSE 并输入许可证文本 。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Copyright (c) 2020 The Python Packaging Authority</span><br><span class="line"></span><br><span class="line">Permission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the &quot;Software&quot;), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and&#x2F;or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:</span><br><span class="line"></span><br><span class="line">The above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.</span><br><span class="line"></span><br><span class="line">THE SOFTWARE IS PROVIDED &quot;AS IS&quot;, WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.</span><br></pre></td></tr></table></figure>

<h3 id="5-生成分发包"><a href="#5-生成分发包" class="headerlink" title="5. 生成分发包"></a>5. 生成分发包</h3><ul>
<li>确保您拥有 setuptools 并 wheel 安装了最新版本：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python -m pip install --user --upgrade setuptools wheel</span><br></pre></td></tr></table></figure></li>
<li>现在从 setup.py 位于的同一目录运行此命令：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python3 setup.py sdist bdist_wheel</span><br></pre></td></tr></table></figure>
此命令应输出大量文本，一旦完成，应在 dist 目录中生成两个文件：<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">dist&#x2F;</span><br><span class="line">  example_pkg_your_username-0.0.1-py3-none-any.whl</span><br><span class="line">  example_pkg_your_username-0.0.1.tar.gz</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h3 id="6-上传-PYPI"><a href="#6-上传-PYPI" class="headerlink" title="6. 上传 PYPI"></a>6. 上传 PYPI</h3><p>最后，是时候将你的包上传到 Python Package Index了！</p>
<p>首先要做的一件事是在 PyPI 上注册一个帐户。<a href="https://pypi.org/%E3%80%82">https://pypi.org/。</a> 记住账号密码，接下来会用到。</p>
<ul>
<li>在该项目命令行下输入<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">twine upload dist&#x2F;*</span><br></pre></td></tr></table></figure>
然后根据提示输入上面创建的账号密码，上传完成后，可以在 PYPI 上查看。</li>
</ul>
<h3 id="7-安装你的包"><a href="#7-安装你的包" class="headerlink" title="7. 安装你的包"></a>7. 安装你的包</h3><p>通过上面的步骤之后，现在你可以在任意的地方安装你的包了，就像安装其他的包一样。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">pip3 install wdhtools</span><br><span class="line">import wdhtools</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>Python</category>
      </categories>
      <tags>
        <tag>pypi</tag>
        <tag>tutorials</tag>
      </tags>
  </entry>
</search>
