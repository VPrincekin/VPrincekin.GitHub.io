<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>BetaGO的博客</title>
  
  <subtitle>AlphaGO的弟弟</subtitle>
  <link href="http://www.betago.com/atom.xml" rel="self"/>
  
  <link href="http://www.betago.com/"/>
  <updated>2020-11-09T08:48:15.921Z</updated>
  <id>http://www.betago.com/</id>
  
  <author>
    <name>Dehong Wang</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>机器学习的问题处理.md</title>
    <link href="http://www.betago.com/2020/11/09/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%97%AE%E9%A2%98%E5%A4%84%E7%90%86-md/"/>
    <id>http://www.betago.com/2020/11/09/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E7%9A%84%E9%97%AE%E9%A2%98%E5%A4%84%E7%90%86-md/</id>
    <published>2020-11-09T08:40:41.000Z</published>
    <updated>2020-11-09T08:48:15.921Z</updated>
    
    
    <summary type="html">在我们面对一个机器学习问题的时候，我们该如何分析处理数据？模型如何选择以及调优？</summary>
    
    
    
    <category term="机器学习" scheme="http://www.betago.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="教程" scheme="http://www.betago.com/tags/%E6%95%99%E7%A8%8B/"/>
    
  </entry>
  
  <entry>
    <title>发生在逻辑回归之前的故事.md</title>
    <link href="http://www.betago.com/2020/10/27/%E5%8F%91%E7%94%9F%E5%9C%A8%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B9%8B%E5%89%8D%E7%9A%84%E6%95%85%E4%BA%8B/"/>
    <id>http://www.betago.com/2020/10/27/%E5%8F%91%E7%94%9F%E5%9C%A8%E9%80%BB%E8%BE%91%E5%9B%9E%E5%BD%92%E4%B9%8B%E5%89%8D%E7%9A%84%E6%95%85%E4%BA%8B/</id>
    <published>2020-10-27T10:00:10.000Z</published>
    <updated>2020-11-03T03:36:06.986Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Gaussian-distribution-高斯分布"><a href="#Gaussian-distribution-高斯分布" class="headerlink" title="Gaussian distribution (高斯分布)"></a>Gaussian distribution (高斯分布)</h2>]]></content>
    
    
    <summary type="html">没有逻辑回归之前，我们是怎么思考一个分类问题呢？</summary>
    
    
    
    <category term="机器学习" scheme="http://www.betago.com/categories/%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="Logistic" scheme="http://www.betago.com/tags/Logistic/"/>
    
  </entry>
  
  <entry>
    <title>深度强化学习-DQN</title>
    <link href="http://www.betago.com/2020/08/24/%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B9%8BDQN/"/>
    <id>http://www.betago.com/2020/08/24/%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B9%8BDQN/</id>
    <published>2020-08-24T07:32:47.000Z</published>
    <updated>2020-08-24T09:35:14.002Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Deep-Q-Network"><a href="#Deep-Q-Network" class="headerlink" title="Deep Q Network"></a>Deep Q Network</h2><h3 id="整体学习流程"><a href="#整体学习流程" class="headerlink" title="整体学习流程"></a>整体学习流程</h3><p>一次循环</p><ul><li>初始化环境</li><li>当前回合 （循环，直到当前回合结束）<ul><li>刷新环境</li><li>根据当前 S， 选择动作 a.(动作选择规则：1.随机选择.2.根据当前 S 在 eval_net 中最大的 a 选择.)</li><li>在环境中采取行为 a, 获得下一个 S_ 和 R, 以及是否终止.</li><li>DQN 存储记忆.(总 memory 大小是固定的, 如果超出总大小, 旧 memory 就被新 memory 替换)</li><li>判断当前是否需要学习 (先累积一些记忆再开始学习):<ul><li>DQN学习:<ul><li>检查是否需要替换 target_net 参数.</li><li>从 memory 中随机抽取 batch_size 记忆</li><li>获取 q_next(target_net 产生的 q)和 q_eval(eval_net 产生的 q). </li><li>q_next, q_eval 包含所有 action 的值, 而我们需要的只是已经选择好的 action 的值, 其他的并不需要. 所以我们将其他的 action 值全变成 0, 将用到的 action 误差值 反向传递回去, 作为更新依据. </li><li>q_target = reward + gamma * max(q_next) (reward 来自于当前批次的 batch_memory)</li><li>根据误差((q_target-q_eval)^2), 训练 eval_net, 得到误差 cost.</li><li>逐渐增加 epsilon, 降低行为的随机性.</li></ul></li></ul></li><li>将 S_ 赋给 S, 继续当前循环.</li><li>判断当前回合是否结束.</li></ul></li></ul><h3 id="loss-function-的构造"><a href="#loss-function-的构造" class="headerlink" title="loss function 的构造"></a>loss function 的构造</h3><p><img src="/2020/08/24/%E6%B7%B1%E5%BA%A6%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E4%B9%8BDQN/dqn_loss_function.png" alt="image"></p>]]></content>
    
    
    <summary type="html">将深度学习与强化学习相结合，提高强化学习的应用场景</summary>
    
    
    
    <category term="强化学习" scheme="http://www.betago.com/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="DQN" scheme="http://www.betago.com/tags/DQN/"/>
    
  </entry>
  
  <entry>
    <title>强化学习入门算法: QLearning &amp; Sarsa</title>
    <link href="http://www.betago.com/2020/08/20/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AE%97%E6%B3%95-QLearning-Sarsa/"/>
    <id>http://www.betago.com/2020/08/20/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E5%85%A5%E9%97%A8%E7%AE%97%E6%B3%95-QLearning-Sarsa/</id>
    <published>2020-08-20T11:34:59.000Z</published>
    <updated>2020-08-21T07:46:07.874Z</updated>
    
    <content type="html"><![CDATA[<h2 id="QLearning-off-policy"><a href="#QLearning-off-policy" class="headerlink" title="QLearning (off-policy)"></a>QLearning (off-policy)</h2><h3 id="整体学习流程："><a href="#整体学习流程：" class="headerlink" title="整体学习流程："></a>整体学习流程：</h3><p>一次循环</p><ul><li><p>初始化环境</p></li><li><p>当前回合 （循环，直到当前回合结束）</p><ul><li><p>根据当前 S， 选择动作 a。(动作选择规则：1.随机选择。2.根据当前 S 最大的 a选择。)</p></li><li><p>在环境中采取行为 a, 获得下一个 S_ 和 R, 以及是否终止。</p></li><li><p>更新 Q Table:</p><p><img src="http://latex.codecogs.com/svg.latex?q_%7B%5C_predict%7D=Q%5Bs,a%5D"></p><p><img src="http://latex.codecogs.com/svg.latex?q_%7B%5C_target%7D=R+%5Cgamma*Q%5Bs%5C_,:%5D_%7Bmax%7D"></p><p><img src="http://latex.codecogs.com/svg.latex?Q(s,a)%5E%7Bnew%7D=Q(s,a)+%5Calpha*(q_%7B%5C_target%7D-q_%7B_predict%7D)"></p><p><img src="http://latex.codecogs.com/svg.latex?Q(s,a)%5E%7Bnew%7D=Q(s,a)+%5Calpha*(R+%5Cgamma*Q%5Bs%5C_,:%5D_%7Bmax%7D-Q%5Bs,a%5D)"></p><p>其中 alpha 代表学习率，来决定这次的误差有多少是要被学习的。gamma 代表对未来奖励的衰减值，来决定这次获得奖励的大小。</p></li><li><p>将 S_ 赋给 S, 继续当前循环。</p></li><li><p>判断当前回合是否结束。</p></li></ul></li></ul><h2 id="Sarsa-on-policy"><a href="#Sarsa-on-policy" class="headerlink" title="Sarsa (on-policy)"></a>Sarsa (on-policy)</h2><h3 id="整体学习流程：-1"><a href="#整体学习流程：-1" class="headerlink" title="整体学习流程："></a>整体学习流程：</h3><p>一次循环</p><ul><li><p>初始化环境</p></li><li><p>根据当前 S， 选择动作 a。(动作选择规则：1.随机选择。2.根据当前 S 最大的 a选择。)</p></li><li><p>当前回合 （循环，直到当前回合结束）</p><ul><li><p>在环境中采取行为 a, 获得下一个 S_ 和 R, 以及是否终止。</p></li><li><p>根据下一个 S_, 选择动作 a_。(动作选择规则同上)</p></li><li><p>更新 Q Table:</p><p><img src="http://latex.codecogs.com/svg.latex?q_%7B%5C_predict%7D=Q%5Bs,a%5D"></p><p><img src="http://latex.codecogs.com/svg.latex?q_%7B%5C_target%7D=R+%5Cgamma*Q%5Bs%5C_,a%5C_%5D"></p><p><img src="http://latex.codecogs.com/svg.latex?Q(s,a)%5E%7Bnew%7D=Q(s,a)+%5Calpha*(q_%7B%5C_target%7D-q_%7B_predict%7D)"></p><p><img src="http://latex.codecogs.com/svg.latex?Q(s,a)%5E%7Bnew%7D=Q(s,a)+%5Calpha*(R+%5Cgamma*Q%5Bs%5C_,a%5C_%5D-Q%5Bs,a%5D)"></p><p>其中 alpha 代表学习率，来决定这次的误差有多少是要被学习的。gamma 代表对未来奖励的衰减值，来决定这次获得奖励的大小。</p></li><li><p>将 S_ 赋给 S, a_ 赋给 a, 继续当前循环。</p></li><li><p>判断当前回合是否结束。</p></li></ul></li></ul><h3 id="与-QLearning-的区别："><a href="#与-QLearning-的区别：" class="headerlink" title="与 QLearning 的区别："></a>与 QLearning 的区别：</h3><ul><li>Sarsa 是说到做到型, 也叫 on-policy, 在线学习, 学着自己在做的事情. 而 QLearning 是说到但并不一定做到, 所以它也叫作 Off-policy, 离线学习. 而因为有了 maxQ, Q-learning 也是一个特别勇敢的算法. Q learning 机器人 永远都会选择最近的一条通往成功的道路, 不管这条路会有多危险. 而 Sarsa 则是相当保守, 他会选择离危险远远的, 完成任务是次要的, 保住自己的小命才是第一位. 这就是使用 Sarsa 方法的不同之处.</li><li>Sarsa 在当前 state 已经想好了 state 对应的 action, 而且想好了下一个 state_ 和下一个 action_ (Qlearning 还没有想好下一个 action_)。更新 Q(s,a) 的时候基于的是下一个 Q(s_, a_)(Qlearning 是基于 maxQ(s_))。</li><li>Sarsa 相对于 Qlearning, 更加的胆小. 因为 Qlearning 永远都是想着 maxQ 最大化, 因为这个 maxQ 而变得贪婪, 不考虑其他非 maxQ 的结果. 我们可以理解成 Qlearning 是一种贪婪, 大胆, 勇敢的算法, 对于错误, 死亡并不在乎. 而 Sarsa 是一种保守的算法, 他在乎每一步决策, 对于错误和死亡比较铭感. 这一点我们会在可视化的部分看出他们的不同. 两种算法都有他们的好处, 比如在实际中, 你比较在乎机器的损害, 用一种保守的算法, 在训练时就能减少损坏的次数.</li></ul><h2 id="Sarsa-lambda-on-policy"><a href="#Sarsa-lambda-on-policy" class="headerlink" title="Sarsa-lambda (on-policy)"></a>Sarsa-lambda (on-policy)</h2><h3 id="整体学习流程：-2"><a href="#整体学习流程：-2" class="headerlink" title="整体学习流程："></a>整体学习流程：</h3><p>一次循环</p><ul><li><p>初始化环境</p></li><li><p>根据当前 S， 选择动作 a。(动作选择规则：1.随机选择。2.根据当前 S 最大的 a选择。)</p></li><li><p>初始化 eligibility_trace(动作跟踪)</p></li><li><p>当前回合 （循环，直到当前回合结束）</p><ul><li><p>在环境中采取行为 a, 获得下一个 S_ 和 R, 以及是否终止。</p></li><li><p>根据下一个 S_, 选择动作 a_。(动作选择规则同上)</p></li><li><p>更新 Q Table:</p><p>对于经历过的 (s, a) state-action, 我们让他+1, 证明他是得到 reward 路途中不可或缺的一环。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># Method 1:</span><br><span class="line">eligibility_trace.loc[s, a] +&#x3D; 1</span><br><span class="line"># Method 2:</span><br><span class="line">eligibility_trace.loc[s, :] *&#x3D; 0</span><br><span class="line">eligibility_trace.loc[s, a] &#x3D; 1</span><br></pre></td></tr></table></figure><p><img src="http://latex.codecogs.com/svg.latex?q_%7B%5C_predict%7D=Q%5Bs,a%5D"></p><p><img src="http://latex.codecogs.com/svg.latex?q_%7B%5C_target%7D=R+%5Cgamma*Q%5Bs%5C_,a%5C_%5D"></p><p><img src="http://latex.codecogs.com/svg.latex?Q_%7Btable%7D%5E%7Bnew%7D=Q_%7Btable%7D+%5Calpha*(q_%7B%5C_target%7D-q_%7B_predict%7D)*eligibility%5C_trace"></p><p><img src="http://latex.codecogs.com/svg.latex?Q_%7Btable%7D%5E%7Bnew%7D=Q_%7Btable%7D+%5Calpha*(R+%5Cgamma*Q%5Bs%5C_,a%5C_%5D-Q%5Bs,a%5D)*eligibility%5C_trace"></p><p>eligibility_trace 的值随着时间衰减, 离获取 reward 越远的步, 他的”不可或缺性”越小。</p><p>eligibility_trace = eligibility_trace * gamma * lambda</p><p>其中 alpha 代表学习率，来决定这次的误差有多少是要被学习的。gamma 代表对未来奖励的衰减值，来决定这次获得奖励的大小。lambda 代表的脚步衰减值，0 为单步更新，1 为回合更新。</p></li><li><p>将 S_ 赋给 S, a_ 赋给 a, 继续当前循环。</p></li><li><p>判断当前回合是否结束。</p></li></ul></li></ul>]]></content>
    
    
    <summary type="html">通过两个简单的算法，了解强化学习的思维逻辑</summary>
    
    
    
    <category term="强化学习" scheme="http://www.betago.com/categories/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0/"/>
    
    
    <category term="QLearning" scheme="http://www.betago.com/tags/QLearning/"/>
    
    <category term="Sarsa" scheme="http://www.betago.com/tags/Sarsa/"/>
    
  </entry>
  
</feed>
